{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Access the Labs and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to our first lab!\n",
    "\n",
    "These labs are going to show you how to use computational linguistics to do the kinds of corpus analysis that we're talking about in *Computational Linguistics for Corpus Analysis*. Even beginners can go through these labs to see computational linguistics in action. If you want more detail, check out the *text_analytics* package that we're using: [https://github.com/jonathandunn/text_analytics](https://github.com/jonathandunn/text_analytics). You can use the package as a source for code examples and best practices. But you can also install the package via pip in order to use it directly:\n",
    "\n",
    "        pip install textanalytics  #Last stable release\n",
    "        pip install git+https://github.com/jonathandunn/text_analytics.git  #Most recent release\n",
    "\n",
    "We're going to start by loading our dependencies. That just means that we start up all the packages that we'll need in order to do the lab. This is how we'll start each and every lab. \n",
    "\n",
    "So, start by **running** the line below. Click on the box and then press the \"Run\" button. (It has an icon that looks like your typical \"play\" icon). You'll notice, if you look at the top-right corner of the screen, that there is an empty circle next to \"Python 3\". When the code is running, that circle will be filled in. So, always wait until the code finishes before moving on. We'll also have the code print something line \"Done!\" every time it finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from text_analytics import TextAnalytics\n",
    "import os\n",
    "import pandas as pd\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Now we have our environment set-up. So, let's load our package now that we've imported it. Here we're telling python that the name *ai* refers to the *text_analytics* package. You can name it anything you'd like, but that's the convention that we will be using. After that, every time we use **ai.something** it is referring to the contents of the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "ai = TextAnalytics()\n",
    "ai.data_dir = os.path.join(\".\", \"data\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these labs we'll be using this *text_analytics* package to do our analysis. This makes it easier for those who don't have as much experience in Python. But, if you are comfortable with Python, the *text_analytics* package also provides examples for how to do everything that we cover in this book in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded everything we need, let's open the data. Run the line below to set the location of our data. The filename is \"economic.congress.1931-2016.gz\" and this first line gives us the path to where we've stored the data. That path doesn't mean anything to you, but it tells the code notebook where to look. Then, we use the *pandas* package to open the data. This is a big corpus, so we only take the first 1,000 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Year  Month Chamber Party  \\\n",
      "0    1931      1       S     R   \n",
      "1    1931      1       S     R   \n",
      "2    1931      1       S     D   \n",
      "3    1931      1       S     R   \n",
      "4    1931      1       S     D   \n",
      "..    ...    ...     ...   ...   \n",
      "995  1931      1       S     D   \n",
      "996  1931      1       S     D   \n",
      "997  1931      1       S     D   \n",
      "998  1931      1       S     D   \n",
      "999  1931      1       S     D   \n",
      "\n",
      "                                                  Text  \n",
      "0    Mr. President. I desire to move at this time. ...  \n",
      "1    Mr. President. in the nature of a memorial. I ...  \n",
      "2    Mr. President. I introduced and had referred t...  \n",
      "3    I ask unanimous consent to have printed in the...  \n",
      "4    Mr. President. during the consideration of the...  \n",
      "..                                                 ...  \n",
      "995  From the Pawnee Agency. at Pawnee. Okla.. I re...  \n",
      "996  I have a telegram from the Muskogee Agency. th...  \n",
      "997  I call attention to a report submitted by the ...  \n",
      "998  It has been in past years. In appropriating mo...  \n",
      "999  The decision makes no distinction between rest...  \n",
      "\n",
      "[1000 rows x 5 columns]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "file = os.path.join(ai.data_dir, \"economic.congress.1931-2016.gz\")\n",
    "df = pd.read_csv(file, nrows = 1000)\n",
    "print(df)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're looking at congressional speeches from the US. The different columns give us information about each speech, like what year it was delivered. And the \"Text\" column gives us the actual data from each sample. Let's take a look at a random selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "ai.print_sample(df)\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll probably notice how messy this looks! That's because we haven't done any cleaning or pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it for our first lab! Today we loaded our dependencies into the environment, created an instance of our *text_analytics* package, and looked at one corpus. Our labs in this course are going to be short and simple like this. You can always play around with the data by changing the code that is written in the cells. In fact, that's a great way to learn how it all works. And, remember, you can always get more details by looking at the *text_analytics* package directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with a few different types of corpora in this book. The first, part of our case-study on *Socio-Economic Indicators*, includes formal speeches to congress and lead paragraphs for articles from *The New York Times*. Both of these corpora cover 1931 to 2016. The congressional corpus contains about 841 million words. The news corpus contains 341 million words (\"economic.nyt.1931-2016.gz\").\n",
    "\n",
    "The second kind of corpus we will be using is published books from the 19th century and early 20th century. We've divided this larger corpus into authors born 1800-1850 and those born 1851-1900. Altogether, this corpus contains about 1,042 million words. Here's one segment: \"stylistics.authorship_1850.gz\".\n",
    "\n",
    "The third family of corpora we will work with is from digital sources: the web and Twitter. This data is geo-referenced, so we know what country for what city each sample is from. These corpora are about 841 million words in total: \"sociolinguistics.english_cities.gz\" and \"sociolinguistics.english_dialects.gz\".\n",
    "\n",
    "These previous corpora represent many different registers, but they are also all English corpora. We get into multi-lingual data with a set of 39 languages with data from three sources: Wikipedia, Twitter, and the web. For each of these 39 languages we have the same amount of data from the same sources. This set of corpora is found in a separate folder: \"\\register\\Register.ara.gz\". Note that the language code is different for each language, but otherwise the naming convention is consistent.\n",
    "\n",
    "Now it's your turn! Use the code box below to load a different data set and print some samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
