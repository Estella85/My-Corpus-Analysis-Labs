{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we're going to use a distance measure, Euclidean distance, to retrieve or find the most similar documents. The idea is that we can represent the content, the style, or the sentiment of a text as a numeric vector. And we can measure the similarity between these vectors as a way to measure to the similarity between the texts they represent. Instead of using a text classifier to distinguish between classes, we'll just find the most similar examples.\n",
    "\n",
    "Let's start by getting our environment ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from text_analytics import TextAnalytics\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ai = TextAnalytics()\n",
    "ai.data_dir = os.path.join(\".\", \"data\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to start by looking at author-based similarity, using style features. So, let's load our data from Project Gutenberg. We don't need the whole dataset, so we only load part of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Author                Title  \\\n",
      "0      abbott_j  alexander_the_great   \n",
      "1      abbott_j  alexander_the_great   \n",
      "2      abbott_j  alexander_the_great   \n",
      "3      abbott_j  alexander_the_great   \n",
      "4      abbott_j  alexander_the_great   \n",
      "...         ...                  ...   \n",
      "1995  bennett_a   the_old_wives_tale   \n",
      "1996  bennett_a   the_old_wives_tale   \n",
      "1997  bennett_a   the_old_wives_tale   \n",
      "1998  bennett_a   the_old_wives_tale   \n",
      "1999  bennett_a   the_old_wives_tale   \n",
      "\n",
      "                                                   Text  \n",
      "0     note project gutenberg also has an html versio...  \n",
      "1     it will be recollected to epirus where her fri...  \n",
      "2     it would be best to endeavor to effect a landi...  \n",
      "3     transport his army across the straits the army...  \n",
      "4     that the true greatness of the soul of alexand...  \n",
      "...                                                 ...  \n",
      "1995  the stipendiary achieved marvellously the illu...  \n",
      "1996  winter overcoat he directed the vast affair of...  \n",
      "1997  after a pause miss insull there are a few card...  \n",
      "1998  in widths lengths and prices she never erred s...  \n",
      "1999  mother would have insisted on by means of tear...  \n",
      "\n",
      "[2000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "file = os.path.join(ai.data_dir, \"stylistics.authorship_1850.gz\")\n",
    "df = pd.read_csv(file, nrows = 2000)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to get ready to find the most similar samples. We're doing to two things here: (1) we transforming the text into vectors representing style and (2) we're choosing a random sample to look at. This means we'll randomly select one chunk and then find the most similar samples to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t338\n",
      "  (0, 1)\t273\n",
      "  (0, 2)\t256\n",
      "  (0, 3)\t125\n",
      "  (0, 4)\t88\n",
      "  (0, 5)\t121\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t115\n",
      "  (0, 8)\t48\n",
      "  (0, 9)\t102\n",
      "  (0, 10)\t25\n",
      "  (0, 11)\t118\n",
      "  (0, 13)\t62\n",
      "  (0, 14)\t28\n",
      "  (0, 15)\t32\n",
      "  (0, 16)\t33\n",
      "  (0, 17)\t18\n",
      "  (0, 18)\t8\n",
      "  (0, 19)\t16\n",
      "  (0, 20)\t16\n",
      "  (0, 21)\t35\n",
      "  (0, 22)\t34\n",
      "  (0, 23)\t4\n",
      "  (0, 24)\t27\n",
      "  (0, 25)\t13\n",
      "  :\t:\n",
      "  (1999, 209)\t2\n",
      "  (1999, 210)\t1\n",
      "  (1999, 212)\t2\n",
      "  (1999, 215)\t1\n",
      "  (1999, 218)\t2\n",
      "  (1999, 220)\t1\n",
      "  (1999, 221)\t4\n",
      "  (1999, 222)\t2\n",
      "  (1999, 224)\t3\n",
      "  (1999, 225)\t1\n",
      "  (1999, 232)\t3\n",
      "  (1999, 236)\t1\n",
      "  (1999, 237)\t1\n",
      "  (1999, 239)\t1\n",
      "  (1999, 242)\t1\n",
      "  (1999, 244)\t1\n",
      "  (1999, 250)\t3\n",
      "  (1999, 252)\t4\n",
      "  (1999, 253)\t1\n",
      "  (1999, 278)\t1\n",
      "  (1999, 281)\t2\n",
      "  (1999, 285)\t1\n",
      "  (1999, 295)\t2\n",
      "  (1999, 301)\t5\n",
      "  (1999, 303)\t1\n",
      "983\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "x, vocab_size = ai.get_features(df, \"style\")\n",
    "sample = random.randint(0,len(df))\n",
    "print(x)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we know what we're looking for. Here's the code. As always, if you want to dig deeper, take a look under the hood in our *text_analytics* package. We're using the *linguistic_distance* function to find the 5 other samples that are most similar to ours. \n",
    "\n",
    "The *x* variable is passing our linguistic features.\n",
    "\n",
    "The *y* variable is passing our meta-data (the author names).\n",
    "\n",
    "The *sample* variable is telling which chunk to use for the similarity search. We can rerun the code block below the generate as many new samples as we'd like.\n",
    "\n",
    "And the *n* variable is telling how many similar examples to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "altsheler_j ['altsheler_j', 'altsheler_j', 'altsheler_j', 'altsheler_j', 'altsheler_j']\n"
     ]
    }
   ],
   "source": [
    "sample = random.randint(0,len(df))\n",
    "y_sample, y_closest = ai.linguistic_distance(x = x, y = df.loc[:,\"Author\"].values, sample = sample, n = 5)\n",
    "print(y_sample, y_closest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it again, with content features. This time we're going to start by loading our pre-trained TF-IDF vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "file_phrases = os.path.join(\".\", ai.data_dir, \"sociolinguistics.english_all.gz\")\n",
    "ai.phrases = ai.deserialize(\"phrases\", file_phrases + \".phrases.json\")\n",
    "ai.tfidf_vectorizer = ai.deserialize(\"tfidf_model\", file_phrases + \".tfidf.json\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can repeat the same code, this time using *content* features. And we'll look at our tweets from different cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            City                                               Text\n",
      "0     washington   you really need to go back to bar tending or ...\n",
      "1         london   jay finley christ in explains why today is co...\n",
      "2          lagos   forget if this happened truly it s definitely...\n",
      "3        toronto   yall i love this skin big thanks to for makin...\n",
      "4        nairobi   the late brilliant prof ali mazrui explains h...\n",
      "...          ...                                                ...\n",
      "1995     atlanta   according to cdc s latest levels of u s flu l...\n",
      "1996       lagos   list of the roads and bridges that buhari is ...\n",
      "1997     calgary   instead of condemning the assault of by a uni...\n",
      "1998     phoenix   also vexing how can we use the experience of ...\n",
      "1999  washington   just landed in the united kingdom heading to ...\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "file = os.path.join(ai.data_dir, \"sociolinguistics.english_cities.gz\")\n",
    "df = pd.read_csv(file, nrows = 2000)\n",
    "print(df)\n",
    "\n",
    "x, vocab_size = ai.get_features(df, \"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "johannesburg ['atlanta', 'johannesburg', 'lagos', 'johannesburg', 'johannesburg']\n"
     ]
    }
   ],
   "source": [
    "sample = random.randint(0,len(df))\n",
    "\n",
    "y_sample, y_closest = ai.linguistic_distance(x = x, y = df.loc[:,\"City\"].values, sample = sample, n = 5)\n",
    "print(y_sample, y_closest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you go! \n",
    "\n",
    "In this lab, we've seen how to find or retrieve the most similar texts using a simple distance measure. We've looked at author style (using books from the 19th century) and content (using tweets). The basic idea here is that texts can be similar to one another in these three different ways.\n",
    "\n",
    "Do distance metrics give results as accurate as the classifiers we've used? Probably not. But remember that they're a lot simpler and they are unsupervised, which means that there isn't any training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Hotel Rating  \\\n",
      "0                 11th Avenue Hotel & Hostel    low   \n",
      "1                                3 West Club   high   \n",
      "2                                  414 Hotel   high   \n",
      "3     70 park avenue hotel - a Kimpton Hotel   high   \n",
      "4       A Victory Inn & Suites Phoenix North    low   \n",
      "...                                      ...    ...   \n",
      "4995                         The Lenox Hotel   high   \n",
      "4996                    The Listel Vancouver   high   \n",
      "4997                         The Loden Hotel   high   \n",
      "4998                            The Lombardy   high   \n",
      "4999                      The Lonsdale Hotel    low   \n",
      "\n",
      "                                                   Text  \n",
      "0     This hostel is in a very good location, close ...  \n",
      "1     We had 5 nights here and were unsure as to wha...  \n",
      "2     This is a small boutique hotel with a nice int...  \n",
      "3     I stayed at 70 Park Ave Hotel the night before...  \n",
      "4     I made a reservation. Cancelled 2 hours later ...  \n",
      "...                                                 ...  \n",
      "4995  Stayed at this hotel with a group of friends o...  \n",
      "4996  We spent a whole week at The Listel in Vancouv...  \n",
      "4997  We stayed at the Loden for the first time and ...  \n",
      "4998  My husband and I both had business in NYC, and...  \n",
      "4999  Let me start by saying, Me and my mate are two...  \n",
      "\n",
      "[5000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "file = os.path.join(ai.data_dir, \"economic.hotels_as_reviews.gz\")\n",
    "df = pd.read_csv(file, nrows = 5000)\n",
    "print(df)\n",
    "\n",
    "x, vocab_size = ai.get_features(df, \"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low ['low', 'low', 'low', 'low', 'low']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "sample = random.randint(0,len(df))\n",
    "\n",
    "y_sample, y_closest = ai.linguistic_distance(x = x, y = df.loc[:,\"Rating\"].values, sample = sample, n = 5)\n",
    "print(y_sample, y_closest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
