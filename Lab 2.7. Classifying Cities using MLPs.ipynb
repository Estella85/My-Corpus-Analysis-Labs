{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we're going to do a content-based classification of cities, based on sample of tweets. This is the same case-study we did in the lab 2.2. But last time we used a Logistic Regression classifier using *scikit-learn*. So this time we're going to solve the same problem using an MLP classifier (multi-layer perceptron) using *tensorflow*.\n",
    "\n",
    "As before, this lab shows the simpler version. If you want to dig more into the details, have a look at the examples in the *text_analytics()* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from text_analytics import TextAnalytics\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ai = TextAnalytics()\n",
    "ai.data_dir = os.path.join(\".\", \"data\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the tweets that we need for each city. We're reducing the corpus to avoid memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             City                                               Text\n",
      "0      washington   you really need to go back to bar tending or ...\n",
      "1          london   jay finley christ in explains why today is co...\n",
      "2           lagos   forget if this happened truly it s definitely...\n",
      "3         toronto   yall i love this skin big thanks to for makin...\n",
      "4         nairobi   the late brilliant prof ali mazrui explains h...\n",
      "...           ...                                                ...\n",
      "64995     toronto   taxing issues federal government to charge gs...\n",
      "64996   bengaluru   this is revealing of americans are held hosta...\n",
      "64997      mumbai   pumps in pumping station did not work in my s...\n",
      "64998     atlanta   why is it that some ppl can speak amp act w i...\n",
      "64999     phoenix   you netflix in already ready for more seasons...\n",
      "\n",
      "[65000 rows x 2 columns]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "file = \"sociolinguistics.english_cities.gz\"\n",
    "file = os.path.join(ai.data_dir, file)\n",
    "df = pd.read_csv(file, nrows=65000)\n",
    "print(df)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now our data is loaded. Let's look at the survey of cities again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "washington 3263\n",
      "london 4154\n",
      "lagos 3057\n",
      "toronto 3268\n",
      "nairobi 2505\n",
      "melbourne 1125\n",
      "perth 946\n",
      "chicago 3614\n",
      "dallas 3013\n",
      "chennai 1230\n",
      "delhi 2506\n",
      "mumbai 2608\n",
      "atlanta 2787\n",
      "bengaluru 2528\n",
      "seattle 1046\n",
      "johannesburg 2772\n",
      "adelaide 968\n",
      "miami 1990\n",
      "calgary 1177\n",
      "boston 3672\n",
      "phoenix 1096\n",
      "auckland 899\n",
      "brisbane 1054\n",
      "karachi 1282\n",
      "new_york 3042\n",
      "los_angeles 2500\n",
      "san_francisco 3113\n",
      "singapore 1743\n",
      "kolkata 1273\n",
      "syndney 769\n"
     ]
    }
   ],
   "source": [
    "freqs = ai.print_labels(df, \"City\")\n",
    "for city in freqs:\n",
    "    print(city, freqs[city])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have (1) our data from each city and (2) our content vectorizer (phrases and TF-IDF weights from the larger digital corpora). We're going to classify these samples by city. The basic code is below; this just called our *text_analytics* package. That package splits the data into training and testing data, learns a classifier, and then evaluates the classifier. We're telling the package to use \"City\" as the ground-truth class, with content features.\n",
    "\n",
    "Let's look at the vocab features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHRASES:\n",
      "['united_states', 'boris_johnson', 'chuck_schumer', 'ricky_gervais', 'rashida_tlaib', 'hillary_clinton', 'devin_nunes', 'sherlock_holmes', 'edinson_cavani', 'climate_change', 'harry_potter', 'happy_birthday']\n",
      "\n",
      "VOCAB\n",
      "['people', 'new', 'today', 'need', 'want', 'video', 'life', 'world', 'year', 'please', 'watch', 'thanks']\n"
     ]
    }
   ],
   "source": [
    "#ai.fit_phrases(df, min_count=1, language=\"en\")\n",
    "#ai.fit_tfidf(df, n_features = 10000)\n",
    "\n",
    "file_phrases = os.path.join(ai.data_dir, \"sociolinguistics.english_all.gz\")\n",
    "ai.phrases = ai.deserialize(\"phrases\", file_phrases + \".phrases.json\")\n",
    "ai.tfidf_vectorizer = ai.deserialize(\"tfidf_model\", file + \".tfidf.json\")\n",
    "\n",
    "print(\"PHRASES:\")\n",
    "example_phrases = list(ai.phrases.phrasegrams.keys())\n",
    "print(example_phrases[:12])\n",
    "\n",
    "print(\"\\nVOCAB\")\n",
    "example_vocab = list(ai.tfidf_vectorizer.vocabulary_.keys())\n",
    "print(example_vocab[:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's different here is the, last time, we used an SVM and this time we're going to use an MLP. The SVM trains all at once. But an MLP trains over time, so we'll see the model's progress as it learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1829/1829 [==============================] - 28s 15ms/step - loss: 0.4906 - accuracy: 0.8519 - val_loss: 0.1837 - val_accuracy: 0.9369\n",
      "Epoch 2/25\n",
      "1829/1829 [==============================] - 21s 12ms/step - loss: 0.0426 - accuracy: 0.9871 - val_loss: 0.2127 - val_accuracy: 0.9348\n",
      "Epoch 3/25\n",
      "1829/1829 [==============================] - 27s 15ms/step - loss: 0.0165 - accuracy: 0.9950 - val_loss: 0.3265 - val_accuracy: 0.9208\n",
      "Epoch 4/25\n",
      "1829/1829 [==============================] - 23s 12ms/step - loss: 0.0155 - accuracy: 0.9949 - val_loss: 0.3027 - val_accuracy: 0.9266\n",
      "Epoch 5/25\n",
      "1829/1829 [==============================] - 21s 12ms/step - loss: 0.0128 - accuracy: 0.9956 - val_loss: 0.3626 - val_accuracy: 0.9206\n",
      "Epoch 6/25\n",
      "1829/1829 [==============================] - 23s 13ms/step - loss: 0.0113 - accuracy: 0.9962 - val_loss: 0.3627 - val_accuracy: 0.9223\n",
      "Epoch 7/25\n",
      "1829/1829 [==============================] - 22s 12ms/step - loss: 0.0100 - accuracy: 0.9964 - val_loss: 0.4246 - val_accuracy: 0.9249\n",
      "Epoch 8/25\n",
      "1829/1829 [==============================] - 30s 17ms/step - loss: 0.0089 - accuracy: 0.9971 - val_loss: 0.3997 - val_accuracy: 0.9257\n",
      "Epoch 9/25\n",
      "1829/1829 [==============================] - 23s 12ms/step - loss: 0.0081 - accuracy: 0.9973 - val_loss: 0.4379 - val_accuracy: 0.9189\n",
      "Epoch 10/25\n",
      "1829/1829 [==============================] - 21s 11ms/step - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.4415 - val_accuracy: 0.9237\n",
      "Epoch 11/25\n",
      "1829/1829 [==============================] - 24s 13ms/step - loss: 0.0058 - accuracy: 0.9981 - val_loss: 0.4909 - val_accuracy: 0.9225\n",
      "Epoch 12/25\n",
      "1829/1829 [==============================] - 25s 14ms/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.4814 - val_accuracy: 0.9254\n",
      "Epoch 13/25\n",
      "1829/1829 [==============================] - 23s 12ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.4702 - val_accuracy: 0.9268\n",
      "Epoch 14/25\n",
      "1829/1829 [==============================] - 25s 14ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.5796 - val_accuracy: 0.9202\n",
      "Epoch 15/25\n",
      "1829/1829 [==============================] - 23s 13ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.5424 - val_accuracy: 0.9271\n",
      "Epoch 16/25\n",
      "1829/1829 [==============================] - 24s 13ms/step - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.5639 - val_accuracy: 0.9274\n",
      "Epoch 17/25\n",
      "1829/1829 [==============================] - 23s 12ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.5830 - val_accuracy: 0.9218\n",
      "Epoch 18/25\n",
      "1829/1829 [==============================] - 20s 11ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.5951 - val_accuracy: 0.9258\n",
      "Epoch 19/25\n",
      "1829/1829 [==============================] - 26s 14ms/step - loss: 0.0044 - accuracy: 0.9985 - val_loss: 0.6079 - val_accuracy: 0.9260\n",
      "Epoch 20/25\n",
      "1829/1829 [==============================] - 23s 13ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.6357 - val_accuracy: 0.9260\n",
      "Epoch 21/25\n",
      "1829/1829 [==============================] - 24s 13ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.6776 - val_accuracy: 0.9222\n",
      "Epoch 22/25\n",
      "1829/1829 [==============================] - 26s 14ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.6842 - val_accuracy: 0.9202\n",
      "Epoch 23/25\n",
      "1829/1829 [==============================] - 25s 14ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.7991 - val_accuracy: 0.9154\n",
      "Epoch 24/25\n",
      "1829/1829 [==============================] - 22s 12ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.6912 - val_accuracy: 0.9238\n",
      "Epoch 25/25\n",
      "1829/1829 [==============================] - 25s 13ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.7759 - val_accuracy: 0.9200\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     adelaide       0.76      0.91      0.83       100\n",
      "      atlanta       0.94      0.91      0.92       263\n",
      "     auckland       0.94      0.95      0.95        85\n",
      "    bengaluru       0.90      0.96      0.93       272\n",
      "       boston       0.94      0.93      0.93       374\n",
      "     brisbane       0.89      0.75      0.81       115\n",
      "      calgary       0.94      0.97      0.95       109\n",
      "      chennai       0.98      0.95      0.96       124\n",
      "      chicago       0.96      0.83      0.89       371\n",
      "       dallas       0.90      0.94      0.91       293\n",
      "        delhi       0.91      0.95      0.93       241\n",
      " johannesburg       1.00      1.00      1.00       277\n",
      "      karachi       0.99      0.99      0.99       113\n",
      "      kolkata       0.93      0.88      0.90       141\n",
      "        lagos       0.99      1.00      1.00       315\n",
      "       london       0.97      0.96      0.97       444\n",
      "  los_angeles       0.81      0.87      0.84       266\n",
      "    melbourne       0.78      0.75      0.77       100\n",
      "        miami       0.91      0.88      0.89       177\n",
      "       mumbai       0.87      0.88      0.88       245\n",
      "      nairobi       1.00      0.99      1.00       233\n",
      "     new_york       0.89      0.84      0.86       312\n",
      "        perth       0.85      0.83      0.84       114\n",
      "      phoenix       0.88      0.87      0.88       109\n",
      "san_francisco       0.96      0.92      0.94       318\n",
      "      seattle       0.86      0.78      0.82       102\n",
      "    singapore       0.99      0.99      0.99       174\n",
      "      syndney       0.86      0.71      0.78        79\n",
      "      toronto       0.92      0.97      0.94       318\n",
      "   washington       0.86      0.99      0.92       316\n",
      "\n",
      "     accuracy                           0.92      6500\n",
      "    macro avg       0.91      0.91      0.91      6500\n",
      " weighted avg       0.92      0.92      0.92      6500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = ai.mlp(df, labels = \"City\", features = \"content\", layers = [500, 500])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Be patient**\n",
    "\n",
    "And there we go! We're looking at the classifier accuracy. \n",
    "\n",
    "This will change a bit from the lecture, because we're using random train/test splits. That means the classifier is looking at different data each time. If you want more advanced examples for how to solve this city classification problem, take a look at the *text_analytics.mlp()* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
