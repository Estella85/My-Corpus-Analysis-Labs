{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we're going to do a style-based classification of authors, based on printed books. This is the same case-study we saw in the lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from text_analytics import TextAnalytics\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ai = TextAnalytics()\n",
    "ai.data_dir = os.path.join(\".\", \"data\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the books that we need for each city. We're looking at authors born between 1850 and 1900, each of whom has a number of books here. That makes sure that we learn to *generalize* so that we're not just predicting individual books. Each sample is about 5k from a book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Author                Title  \\\n",
      "0      abbott_j  alexander_the_great   \n",
      "1      abbott_j  alexander_the_great   \n",
      "2      abbott_j  alexander_the_great   \n",
      "3      abbott_j  alexander_the_great   \n",
      "4      abbott_j  alexander_the_great   \n",
      "...         ...                  ...   \n",
      "15994    wood_h       victor_serenus   \n",
      "15995    wood_h       victor_serenus   \n",
      "15996    wood_h       victor_serenus   \n",
      "15997    wood_h       victor_serenus   \n",
      "15998    wood_h       victor_serenus   \n",
      "\n",
      "                                                    Text  \n",
      "0      note project gutenberg also has an html versio...  \n",
      "1      it will be recollected to epirus where her fri...  \n",
      "2      it would be best to endeavor to effect a landi...  \n",
      "3      transport his army across the straits the army...  \n",
      "4      that the true greatness of the soul of alexand...  \n",
      "...                                                  ...  \n",
      "15994  since i have been with amabel it hath waxed st...  \n",
      "15995  his face uttered a loud cry and shrank back af...  \n",
      "15996  the taurus mountains made the afternoon balmy ...  \n",
      "15997  me to a place not very far distant where all m...  \n",
      "15998  never knew these things before now thou dost r...  \n",
      "\n",
      "[15999 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "file = \"stylistics.authorship_1850.gz\"\n",
    "file = os.path.join(ai.data_dir, file)\n",
    "df = pd.read_csv(file)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use function word n-grams. These are pre-defined, so we don't need to fit a model before we use them. Let's take a look at how many authors we have here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abbott_j 927\n",
      "altsheler_j 610\n",
      "bennett_a 573\n",
      "bindloss_h 815\n",
      "chambers_r 728\n",
      "collingwood_h 659\n",
      "collins_w 858\n",
      "crawford_f 912\n",
      "doyle_a 694\n",
      "galsworthy_j 337\n",
      "gissing_g 528\n",
      "haggard_h 956\n",
      "hume_f 975\n",
      "london_j 590\n",
      "meade_l 701\n",
      "oppenheim_e 848\n",
      "parker_g 429\n",
      "quiller-couch_a 514\n",
      "stratemeyer_e 792\n",
      "ward_h 671\n",
      "warner_c 300\n",
      "wells_c 515\n",
      "weyman_s 511\n",
      "wood_h 556\n"
     ]
    }
   ],
   "source": [
    "freq = ai.print_labels(df, \"Author\")\n",
    "\n",
    "for author in freq:\n",
    "    print(author, freq[author])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lot of samples per author. So it is going to be difficult to make generalizations that tell us who wrote what using just function word n-grams like \"there was\" or \"in the\". So now we have (1) our data from Project Gutenberg and (2) our style vectorizer (function word n-grams). We're going to classify these by author. The basic code is below; this just called our *text_analytics* package. That package splits the data into training and testing data, learns a classifier, and then evaluates the classifier. We're telling the package to use \"Author\" as the ground-truth class, with stylistic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       abbott_j       0.99      0.98      0.98        96\n",
      "    altsheler_j       0.98      1.00      0.99        50\n",
      "      bennett_a       1.00      1.00      1.00        61\n",
      "     bindloss_h       0.99      0.99      0.99        96\n",
      "     chambers_r       0.97      1.00      0.98        65\n",
      "  collingwood_h       1.00      0.98      0.99        55\n",
      "      collins_w       0.98      0.98      0.98        95\n",
      "     crawford_f       0.99      0.99      0.99        94\n",
      "        doyle_a       1.00      1.00      1.00        62\n",
      "   galsworthy_j       1.00      1.00      1.00        27\n",
      "      gissing_g       0.98      0.97      0.97        59\n",
      "      haggard_h       0.99      0.98      0.98        99\n",
      "         hume_f       0.99      1.00      1.00       107\n",
      "       london_j       1.00      1.00      1.00        64\n",
      "        meade_l       1.00      0.99      0.99        70\n",
      "    oppenheim_e       1.00      1.00      1.00        75\n",
      "       parker_g       0.98      1.00      0.99        40\n",
      "quiller-couch_a       0.96      1.00      0.98        46\n",
      "  stratemeyer_e       0.99      1.00      0.99        79\n",
      "         ward_h       0.97      0.97      0.97        66\n",
      "       warner_c       0.97      0.91      0.94        35\n",
      "        wells_c       0.98      0.98      0.98        46\n",
      "       weyman_s       1.00      0.98      0.99        54\n",
      "         wood_h       1.00      1.00      1.00        59\n",
      "\n",
      "       accuracy                           0.99      1600\n",
      "      macro avg       0.99      0.99      0.99      1600\n",
      "   weighted avg       0.99      0.99      0.99      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = ai.shallow_classification(df, labels = \"Author\", features = \"style\", classifier = \"lm\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Be patient**\n",
    "\n",
    "And there we go! We're looking at the classifier accuracy. \n",
    "\n",
    "This will change a bit from the lecture, because we're using random train/test splits. That means the classifier is looking at different data each time. If you want more advanced examples for how to solve this authorship classification problem, take a look at the *text_analytics.shallow_classification()* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
