{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we're going to use positional vectors to predict what part-of-speech a word belongs to. A *positional vector* encodes context within a sentence while the other vectors we've used have encoded specific features regardless of their position. We start, as always, by loading our dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from text_analytics import TextAnalytics\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ai = TextAnalytics()\n",
    "ai.data_dir = os.path.join(\".\", \"data\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we're going to work with corpora from the Universal Dependencies project (universaldependencies.org). We've modified the original corpora, but the format is quite similar. Each word is a row. The word-form is one column. And the part-of-speech tag is another column. We have sentence information as well; this let's us make sure that we don't encode sequences which cross a sentence boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Source  Sentence_ID Word_ID    Word    POS\n",
      "0       en_ewt            1       1    from    ADP\n",
      "1       en_ewt            1       2     the    DET\n",
      "2       en_ewt            1       3      ap  PROPN\n",
      "3       en_ewt            1       4   comes   VERB\n",
      "4       en_ewt            1       5    this    DET\n",
      "...        ...          ...     ...     ...    ...\n",
      "559457  en_pud        32356      21       a    DET\n",
      "559458  en_pud        32356      22  friend   NOUN\n",
      "559459  en_pud        32356      23      of    ADP\n",
      "559460  en_pud        32356      24   peace   NOUN\n",
      "559461  en_pud        32356      25       .  PUNCT\n",
      "\n",
      "[559462 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "file = os.path.join(ai.data_dir, \"syntax.pos_english.gz\")\n",
    "df = pd.read_csv(file)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataframe for just one sentence to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Source  Sentence_ID Word_ID         Word    POS\n",
      "7   en_ewt            2       1    president  PROPN\n",
      "8   en_ewt            2       2         bush  PROPN\n",
      "9   en_ewt            2       3           on    ADP\n",
      "10  en_ewt            2       4      tuesday  PROPN\n",
      "11  en_ewt            2       5    nominated   VERB\n",
      "12  en_ewt            2       6          two    NUM\n",
      "13  en_ewt            2       7  individuals   NOUN\n",
      "14  en_ewt            2       8           to   PART\n",
      "15  en_ewt            2       9      replace   VERB\n",
      "16  en_ewt            2      10     retiring   VERB\n",
      "17  en_ewt            2      11      jurists   NOUN\n",
      "18  en_ewt            2      12           on    ADP\n",
      "19  en_ewt            2      13      federal    ADJ\n",
      "20  en_ewt            2      14       courts   NOUN\n",
      "21  en_ewt            2      15           in    ADP\n",
      "22  en_ewt            2      16          the    DET\n",
      "23  en_ewt            2      17   washington  PROPN\n",
      "24  en_ewt            2      18         area   NOUN\n",
      "25  en_ewt            2      19            .  PUNCT\n"
     ]
    }
   ],
   "source": [
    "test_df = df.loc[df.loc[:,\"Sentence_ID\"]==2]\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code below will iterate over each word in the sentence and create a positional vector for that specific word. The vector contains two words before and two words after. We then print out the vector and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', '#', 'president', 'bush', 'on']\n",
      "['#', 'president', 'bush', 'on', 'tuesday']\n",
      "['president', 'bush', 'on', 'tuesday', 'nominated']\n",
      "['bush', 'on', 'tuesday', 'nominated', 'two']\n",
      "['on', 'tuesday', 'nominated', 'two', 'individuals']\n",
      "['tuesday', 'nominated', 'two', 'individuals', 'to']\n",
      "['nominated', 'two', 'individuals', 'to', 'replace']\n",
      "['two', 'individuals', 'to', 'replace', 'retiring']\n",
      "['individuals', 'to', 'replace', 'retiring', 'jurists']\n",
      "['to', 'replace', 'retiring', 'jurists', 'on']\n",
      "['replace', 'retiring', 'jurists', 'on', 'federal']\n",
      "['retiring', 'jurists', 'on', 'federal', 'courts']\n",
      "['jurists', 'on', 'federal', 'courts', 'in']\n",
      "['on', 'federal', 'courts', 'in', 'the']\n",
      "['federal', 'courts', 'in', 'the', 'washington']\n",
      "['courts', 'in', 'the', 'washington', 'area']\n",
      "['in', 'the', 'washington', 'area', '.']\n",
      "['the', 'washington', 'area', '.', '#']\n",
      "['washington', 'area', '.', '#', '#']\n"
     ]
    }
   ],
   "source": [
    "x_vectors = []\n",
    "y_vector = []\n",
    "\n",
    "#A list of words and a list of ground-truth labels\n",
    "words = test_df.loc[:,\"Word\"].values\n",
    "tags = test_df.loc[:,\"POS\"].values\n",
    "            \n",
    "#Create a positional vector for each word in the sentence\n",
    "for i in range(len(words)):\n",
    "    y_vector.append(tags[i])\n",
    "    vector = []\n",
    "                \n",
    "    #Find the correct context window, filling in slots at the edges\n",
    "    for j in [-2, -1, 0, 1, 2]:\n",
    "        if i+j < 0 or i+j > len(words)-1:\n",
    "            vector.append(\"#\")\n",
    "        else:\n",
    "            vector.append(words[i+j])\n",
    "                        \n",
    "    #Save the positional vector for this word\n",
    "    print(vector)\n",
    "    x_vectors.append(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, these sequences are just arrays of strings. We need to convert this into a one-hot encoding in which each position has a unique column in the vector. Here we convert these strings into one-hot encodings and then we print out the complete arrays to show how they look. Each row (each new array) represents just one word and its surrounding context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#With all sentences finished, conert into numpy array\n",
    "x_vectors = np.array(x_vectors)\n",
    "y_vector = np.array(y_vector)\n",
    "\n",
    "#Convert into a one-hot encoding\n",
    "encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "encoder.fit(x_vectors)\n",
    "        \n",
    "#Return the x and y vectors\n",
    "x_vectors = encoder.transform(x_vectors)\n",
    "print(x_vectors.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does this work for classifying parts-of-speech? Let's find out!\n",
    "\n",
    "The line below will use the *text_analytics* package to create a positional one-hot encoding for about half a million words and predict part-of-speech tags using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.89      0.84      0.87      3693\n",
      "         ADP       0.93      0.97      0.95      5358\n",
      "         ADV       0.90      0.81      0.85      2627\n",
      "         AUX       0.95      0.98      0.97      3071\n",
      "       CCONJ       0.99      0.99      0.99      1753\n",
      "         DET       0.98      0.97      0.97      4672\n",
      "        INTJ       0.96      0.72      0.82       174\n",
      "        NOUN       0.86      0.95      0.90      9923\n",
      "         NUM       0.95      0.84      0.89       965\n",
      "        PART       0.95      0.98      0.96      1450\n",
      "        PRON       0.96      0.96      0.96      4612\n",
      "       PROPN       0.84      0.75      0.79      3167\n",
      "       PUNCT       0.99      1.00      1.00      6659\n",
      "       SCONJ       0.86      0.76      0.80      1050\n",
      "         SYM       0.88      0.68      0.77       118\n",
      "        VERB       0.92      0.91      0.92      5988\n",
      "           X       0.91      0.28      0.42       156\n",
      "           _       0.97      0.98      0.97       511\n",
      "\n",
      "    accuracy                           0.93     55947\n",
      "   macro avg       0.93      0.85      0.88     55947\n",
      "weighted avg       0.93      0.93      0.92     55947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = ai.pos_tagger(df, classifier=\"lm\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are always methods we can use to improve our tagging performance. But this lab shows how a simple two-word context window can be used to classify the syntactic properties of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
