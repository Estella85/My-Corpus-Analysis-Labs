{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we're going to use a different approach to similarity: finding the association between words. This is the same idea that we used for finding phrases (Pointwise Mutual Information). But here we will use a direction-specific measure to provide more syntactically-sensitive results.\n",
    "\n",
    "Let's start by getting our environment ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from text_analytics import TextAnalytics\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ai = TextAnalytics()\n",
    "ai.data_dir = os.path.join(\".\", \"data\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to start by looking at our data from Project Gutenberg. We don't need the whole dataset, so we only load part of it. That's because we're going to test the association measure function. Later we will load the complete results already processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Author                Title  \\\n",
      "0       abbott_j  alexander_the_great   \n",
      "1       abbott_j  alexander_the_great   \n",
      "2       abbott_j  alexander_the_great   \n",
      "3       abbott_j  alexander_the_great   \n",
      "4       abbott_j  alexander_the_great   \n",
      "..           ...                  ...   \n",
      "995  altsheler_j        the_candidate   \n",
      "996  altsheler_j        the_candidate   \n",
      "997  altsheler_j        the_candidate   \n",
      "998  altsheler_j        the_candidate   \n",
      "999  altsheler_j        the_candidate   \n",
      "\n",
      "                                                  Text  \n",
      "0    note project gutenberg also has an html versio...  \n",
      "1    it will be recollected to epirus where her fri...  \n",
      "2    it would be best to endeavor to effect a landi...  \n",
      "3    transport his army across the straits the army...  \n",
      "4    that the true greatness of the soul of alexand...  \n",
      "..                                                 ...  \n",
      "995  comrades yet churchill was not wholly pleased ...  \n",
      "996  idea and he immediately hunted up the cousin a...  \n",
      "997  station and presently she met king plummer com...  \n",
      "998  be in to breakfast you have diagnosed his chie...  \n",
      "999  to mitigate things stepped forward i beg your ...  \n",
      "\n",
      "[1000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "file = os.path.join(ai.data_dir, \"stylistics.authorship_1850.gz\")\n",
    "df = pd.read_csv(file, nrows = 1000)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to get ready to find the most associated pairs of words. The *save_phraser* option allows us to use this as the basis for our phrase detection as well. But here we will just inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTime: 227.36381030082703 Full: 989787  Reduced: 392987 with 4997883 words.\n",
      "\n",
      "\tTOTAL TIME: 227.37843680381775\n",
      "\tTOTAL NGRAMS: 392987\n",
      "\tTOTAL WORDS: 4997883\n",
      "\tAfter pruning:\n",
      "\tTOTAL NGRAMS: 382815\n",
      "\n",
      "\tCalculating association for 382815 pairs.\n",
      "\tProcessed 382815 items in 1.6508512496948242\n",
      "         Word1    Word2       Max        LR        RL  Freq\n",
      "347524      de  peyster  0.998835  0.998835  0.063444   126\n",
      "57412    madam   rachel  0.998102  0.998102  0.414062    53\n",
      "241275  madame   campan  0.997126  0.997126  0.039785    37\n",
      "290489    loch    leven  0.997059  0.997059  0.447368    34\n",
      "24262    rhode   island  0.997006  0.031223  0.997006    36\n",
      "...        ...      ...       ...       ...       ...   ...\n",
      "59263      and      and -0.036657 -0.036657 -0.036657   127\n",
      "4039       the       of -0.037537 -0.068729 -0.037537  1891\n",
      "16027      the      and -0.037744 -0.075858 -0.037744   433\n",
      "36766       of       of -0.041107 -0.041107 -0.041107    47\n",
      "3216       the      the -0.078698 -0.078698 -0.078698  1047\n",
      "\n",
      "[348964 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "association_df = ai.get_association(df, min_count = 1, save_phraser = True)\n",
    "print(association_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a direction-specific measure, so we have two basic numbers: left-to-right and right-to-left. We also have a *max* column that allows us to find the best direction for each pair. Finally, we have frequency. This measure is not as frequency-sensitive as the PMI, but there is still a bias toward infrequent pairs (which will be highly associated).\n",
    "\n",
    "Now let's load fully trained word pairs for the Project Gutenberg dataset and the web/tweet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word1      Word2       Max        LR        RL    Freq\n",
      "0          le    gardeur  0.999871  0.999871  0.022920    1344\n",
      "1           m     kinlay  0.999788  0.999788  0.010918     638\n",
      "2         des  esseintes  0.999738  0.999738  0.011821     442\n",
      "3         des    hermies  0.999672  0.999672  0.009146     342\n",
      "4         don     an√≠bal  0.999594  0.999594  0.002330     464\n",
      "...       ...        ...       ...       ...       ...     ...\n",
      "3987819   the        and -0.032834 -0.061111 -0.032834  182982\n",
      "3987820   the         of -0.032943 -0.059793 -0.032943  235270\n",
      "3987821   and        and -0.033293 -0.033293 -0.033293   44364\n",
      "3987822    of         of -0.035106 -0.035106 -0.035106   15540\n",
      "3987823   the        the -0.067346 -0.067346 -0.067346   87214\n",
      "\n",
      "[3987824 rows x 6 columns]\n",
      "           Word1        Word2       Max        LR        RL   Freq\n",
      "0        gustado           un  0.999816  0.014754  0.999816    870\n",
      "1         indigo       cafind  0.999745  0.999745  0.129993    397\n",
      "2          votes  dullaverage  0.999718  0.999718  0.008584    456\n",
      "3        cookies       okread  0.999666  0.999666  0.021476    316\n",
      "4         rodong       sinmun  0.999636  0.999636  0.975177    275\n",
      "...          ...          ...       ...       ...       ...    ...\n",
      "4547271      the           of -0.021413 -0.041590 -0.021413  14004\n",
      "4547272      the          and -0.021730 -0.041168 -0.021730  22209\n",
      "4547273       to           to -0.023303 -0.023303 -0.023303  77309\n",
      "4547274      the           to -0.026867 -0.041895 -0.026867  15820\n",
      "4547275      the          the -0.042298 -0.042298 -0.042298  33977\n",
      "\n",
      "[4547276 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "file = os.path.join(ai.data_dir, \"stylistics.gutenberg_all.gz.association.gz\")\n",
    "pg_df = pd.read_csv(file)\n",
    "print(pg_df)\n",
    "\n",
    "file = os.path.join(ai.data_dir, \"sociolinguistics.english_all.gz.association.gz\")\n",
    "tw_df = pd.read_csv(file)\n",
    "print(tw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some samples of very associated left-to-right phrases from both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Word1      Word2       Max        LR        RL   Freq\n",
      "422    sir    cropton  0.993883  0.993883  0.000029     18\n",
      "215    van     burnam  0.996390  0.996390  0.013843    558\n",
      "1449    on  horseback  0.931131  0.931131  0.002727  16716\n",
      "1765  miss      meeke  0.920657  0.920657  0.000340    140\n",
      "741     de     tabley  0.991285  0.991285  0.000026     12\n",
      "...    ...        ...       ...       ...       ...    ...\n",
      "360     mr     selwin  0.994804  0.994804  0.000087     20\n",
      "970     of     thunes  0.963883  0.963883  0.000002     56\n",
      "829    van      vliet  0.982720  0.982720  0.002828    114\n",
      "68     van    someren  0.998297  0.998297  0.001489     60\n",
      "886   pont    audemer  0.974357  0.974357  0.029851     76\n",
      "\n",
      "[1607 rows x 6 columns]\n",
      "                Word1                Word2       Max        LR        RL  Freq\n",
      "1912           younus              algohar  0.986111  0.986111  0.250000   142\n",
      "813            yvonne             fovargue  0.993786  0.993786  0.008060    16\n",
      "252            ovidiu              hategan  0.996885  0.996885  0.780488    32\n",
      "3527                o                rishe  0.956171  0.956171  0.000224    66\n",
      "286               nux               vomica  0.996678  0.996678  0.370370    30\n",
      "...               ...                  ...       ...       ...       ...   ...\n",
      "2195  homexpressabout  meappreciatecontact  0.981481  0.981481  0.779412    53\n",
      "5320           daniel              kaluuya  0.923048  0.923048  0.001474    36\n",
      "3487           dondup               dqueen  0.956521  0.956521  0.092437    22\n",
      "5653             joni            eareckson  0.916666  0.916666  0.017828    11\n",
      "3493       registrant             notshown  0.956520  0.956520  0.013724    22\n",
      "\n",
      "[3730 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "pg_lr_df = pg_df.loc[pg_df[\"LR\"] > 0.90].sample(frac = 1)\n",
    "print(pg_lr_df)\n",
    "\n",
    "tw_lr_df = tw_df.loc[tw_df[\"LR\"] > 0.90].sample(frac = 1)\n",
    "print(tw_lr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can repeat the same code, this time finding right-to-left phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Word1        Word2       Max            LR        RL  Freq\n",
      "1469              merrion       square  0.930160  1.064107e-03  0.930160    80\n",
      "1812               faeroe        isles  0.916660  3.109978e-03  0.916660    22\n",
      "2040               tuatha           de  0.901989  1.577325e-04  0.901989    74\n",
      "1926              metonic        cycle  0.909088  5.704505e-03  0.909088    20\n",
      "642         heterochronic  variability  0.991732  3.276898e-03  0.991732    12\n",
      "...                   ...          ...       ...           ...       ...   ...\n",
      "290                  fadl           el  0.995464  1.841312e-03  0.995464    22\n",
      "192                 menlo         park  0.996633  6.475565e-04  0.996633    30\n",
      "323                  t√≥th        j√°nos  0.995025  2.040816e-01  0.995025    20\n",
      "1124               geroch           of  0.957401  3.352864e-07  0.957401    12\n",
      "1163  contradistinguished         from  0.953139  2.004467e-05  0.953139    90\n",
      "\n",
      "[475 rows x 6 columns]\n",
      "                Word1                                              Word2  \\\n",
      "2818         centeris                                               your   \n",
      "1251         jyllands                                             posten   \n",
      "3499           dofree                                        attractions   \n",
      "731   lankasudansudan  southsurinameswazilandswedenswitzerlandsyriata...   \n",
      "2943            karka                                          capricorn   \n",
      "...               ...                                                ...   \n",
      "4022       commentyou                                                may   \n",
      "3575     searchorders                                                and   \n",
      "3887             musn                                                  t   \n",
      "6231          minored                                                 in   \n",
      "25        interiorscw                                      interiorsgood   \n",
      "\n",
      "           Max            LR        RL  Freq  \n",
      "2818  0.969551  3.057503e-05  0.969551   112  \n",
      "1251  0.991736  6.315789e-01  0.991736    12  \n",
      "3499  0.956510  2.294773e-03  0.956510    22  \n",
      "731   0.994152  9.941520e-01  0.944444    17  \n",
      "2943  0.967293  1.465792e-01  0.967293   917  \n",
      "...        ...           ...       ...   ...  \n",
      "4022  0.946596  2.316069e-04  0.946596   199  \n",
      "3575  0.954726  2.273878e-06  0.954726    41  \n",
      "3887  0.948871  1.483856e-05  0.948871    62  \n",
      "6231  0.906784  8.760135e-07  0.906784    12  \n",
      "25    0.999124  9.827586e-01  0.999124   114  \n",
      "\n",
      "[2875 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "pg_rl_df = pg_df.loc[pg_df[\"RL\"] > 0.90].sample(frac = 1)\n",
    "print(pg_rl_df)\n",
    "\n",
    "tw_rl_df = tw_df.loc[tw_df[\"RL\"] > 0.90].sample(frac = 1)\n",
    "print(tw_rl_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you go! \n",
    "\n",
    "In this lab, we've seen how to find or retrieve the most associated words, while taking into account the direction of association. Since we've randomly shuffled the datasets, you can repeat the cells to show more examples. These measure pick up more specific patterns than the PMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
