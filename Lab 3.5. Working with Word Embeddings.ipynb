{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work with word embeddings. We've been focusing on texts for the most part, but now we'll look at words directly using word2vec. We'll see what these embeddings look like and then use them to explore word similarity.\n",
    "\n",
    "Let's start by getting our environment ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from text_analytics import TextAnalytics\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ai = TextAnalytics()\n",
    "ai.data_dir = os.path.join(\".\", \"data\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, learning word embeddings takes quite a long time, even longer than the tasks we've been doing so far. Because of that, we'll load pre-trained word embeddings. If you *did* want to do it yourself, here's the code, commented out. And you can see a more detailed recipe in the *text_analytics* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1235968   0.04748945  0.16285081 ...  0.03241363 -0.03300684\n",
      "   0.26601192]\n",
      " [-0.0030584   0.04572405  0.10358463 ... -0.00173236 -0.10827369\n",
      "   0.20938973]\n",
      " [-0.19914334  0.03242941  0.0766376  ...  0.12763536  0.02289162\n",
      "   0.0425519 ]\n",
      " ...\n",
      " [ 0.05791736  0.24080583  0.11852352 ...  0.13066971  0.03170295\n",
      "   0.20842037]\n",
      " [-0.05558461  0.00972249  0.02807915 ...  0.00696762  0.19038115\n",
      "   0.08795328]\n",
      " [ 0.04808212  0.17134945 -0.06976075 ...  0.02019391  0.03982212\n",
      "   0.0915439 ]]\n",
      "['the_DET', 'of_ADP', 'a_DET', 'and_CCONJ', 'in_ADP', 'number_NOUN', 'to_PART', 'to_ADP', 'for_ADP', 'on_ADP', 'at_ADP', 'is_AUX', 'by_ADP', 'was_AUX', 'with_ADP', 'that_SCONJ', 'as_SCONJ', 'his_DET', 'from_ADP', 'it_PRON']\n"
     ]
    }
   ],
   "source": [
    "file = \"economic.nyt.1931-2016.gz\"\n",
    "file = os.path.join(ai.data_dir, file)\n",
    "#df = pd.read_csv(file)\n",
    "#print(df)\n",
    "\n",
    "#ai.train_word2vec(df)\n",
    "\n",
    "ai.word_vectors = ai.deserialize(\"w2v_embedding\", file + \".w2v_embedding.json\")\n",
    "ai.word_vectors_vocab = ai.deserialize(\"w2v_vocab\", file + \".w2v_vocab.json\")\n",
    "    \n",
    "print(ai.word_vectors)\n",
    "print(list(ai.word_vectors_vocab.keys())[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now loaded the embeddings for the vocabulary. These embeddings are trained on lead paragraphs from *The New York Times* from 1931 to 2016. Now let's get them ready to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build an index of each word\n",
    "y = list(ai.word_vectors_vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of our vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ã©minence_NOUN', 'mezzo_ADJ', 'marrieds_NOUN', 'kevles_PROPN', 'garner_PROPN', 'rebar_NOUN', 'nolting_VERB', 'margarette_PROPN', 'liveaction_PROPN', 'kusz_PROPN']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sample = random.sample(ai.word_vectors_vocab.keys(), 10)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice right away that these aren't just plain words! There are tags like NOUN and VERB. What's going on?\n",
    "\n",
    "First, we used PMI to find phrases *before* we trained the word embeddings. For example, in the random sample I'm looking at, we have \"downton abbey\". This is a single entity, and using PMI here is a way to capture that these words together have a different meaning than these words on their own.\n",
    "\n",
    "Second, we have these tags like NOUN and VERB. These are part-of-speech tags, or pos-tags for short. This tells us the syntactic category for each word. We got these using the *spaCy* package. You can see how by looking at the *train_word2vec* and *clean* functions in our *text_analytics* package. This gives us the categories below, that you probably have heard about before. If not, you can read more about them [here](https://universaldependencies.org/u/pos/all.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Open-class words  |  Closed-class words  |  Other  |\n",
    "| :------------- | :----------:            | ------: |\n",
    "|ADJ (adjective)         | ADP (preposition) \t                   | PUNCT (punctuation)  |\n",
    "|ADV (adverb)\t         | AUX (auxiliary verb)\t                   | SYM (symbol)    |\n",
    "|INTJ (interjection)         | CCONJ (coordinating conjunction)                   | X  (misc)     |\n",
    "|NOUN (noun) \t         | DET (determiner, like \"the\")\t                   |         |\n",
    "|PROPN (proper noun)      | NUM (number)\t                   |         |\n",
    "|VERB (verb)\t         | PART  (participle)                  |         |\n",
    "                 | PRON (pronoun)                   |         |\n",
    "\t             | SCONJ (subordinating conjunction)                   |\t     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a word of explanation here: *open-class words* are an infinite set for every language. We can always make up new nouns, for example. So these tags are what we would have to learn to apply to words that we've never seen before. But *closed-class words* are limited, are fixed. For example, all the closed-class words in English are in our list of function words. So, we already know something about those words. And we will always have seen all the closed-class words that there are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, why did we add these syntactic tags before we learned word embeddings? This is the easiest way to disambiguate words. In the lecture, we talked about words like \"table\" that can be either a noun or a verb. And they have very different meanings, sometimes completely unrelated, in each form. So, we can add the part-of-speech tags to keep track of which word sense we're dealing with.\n",
    "\n",
    "Now let's do some coding! The block below will randomly choose one word, show us that word, and then show us the five most similar words. *Similarity* here is calculated using cosine distance, applied to words instead of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wildlings_NOUN ['cyclamens_NOUN', 'kalanchoes_NOUN', 'summerflowering_NOUN', 'bellflowers_NOUN', 'espaliers_NOUN']\n"
     ]
    }
   ],
   "source": [
    "sample = random.sample(ai.word_vectors_vocab.keys(), 1)[0]\n",
    "index = ai.word_vectors_vocab[sample]\n",
    "sample, closest = ai.linguistic_distance(x = ai.word_vectors, y = y, sample = index, n = 5, metric=\"cosine\")\n",
    "\n",
    "print(sample, closest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you go! Every time you re-run the code block you will see a different set of similar words.\n",
    "\n",
    "Let's load word embeddings from the corpus of congressional speechs the covers the same time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = \"economic.congress.1931-2016.gz\"\n",
    "file2 = os.path.join(ai.data_dir, file2)\n",
    "\n",
    "congress_word_vectors = ai.deserialize(\"w2v_embedding\", file2 + \".w2v_embedding.json\")\n",
    "congress_word_vectors_vocab = ai.deserialize(\"w2v_vocab\", file2 + \".w2v_vocab.json\")\n",
    "\n",
    "#Build an index of each word\n",
    "congress_y = list(congress_word_vectors_vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's choose a word from one dataset and, if it is present in both, find out the most similar words as computed from the different corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oats_VERB\n",
      "\n",
      " NYT oats_VERB ['corn_VERB', 'wheat_VERB', 'soybeans_VERB', 'oats_NOUN', 'soybeans_NOUN', 'soy_NOUN', 'rye_VERB', 'oldcrop_PROPN', 'corn_NOUN', 'lard_NOUN']\n",
      "\n",
      " Congress oats_VERB ['barley_NOUN', 'rye_NOUN', 'grains_VERB', 'flaxseed_VERB', 'sorghum_NOUN', 'soybeans_NOUN', 'grains_NOUN', 'oats_NOUN', 'oats_ADJ', 'sorghums_NOUN']\n"
     ]
    }
   ],
   "source": [
    "sample = random.sample(ai.word_vectors_vocab.keys(), 1)[0]\n",
    "index = ai.word_vectors_vocab[sample]\n",
    "\n",
    "if sample in congress_word_vectors_vocab:\n",
    "    print(sample)\n",
    "    congress_index = congress_word_vectors_vocab[sample]\n",
    "    \n",
    "    sample, closest = ai.linguistic_distance(x = ai.word_vectors, y = y, sample = index, n = 10, metric=\"cosine\")\n",
    "    print(\"\\n\", \"NYT\", sample, closest)\n",
    "    \n",
    "    sample, closest = ai.linguistic_distance(x = congress_word_vectors, y = congress_y, sample = congress_index, n = 10, metric=\"cosine\")\n",
    "    print(\"\\n\", \"Congress\", sample, closest)\n",
    "    \n",
    "else:\n",
    "    print(\"Try again! This word is not found in both corpora.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
