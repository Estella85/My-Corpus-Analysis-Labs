{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we're going to do a content-based classification of cities, based on tweets. This is the same case-study we saw in the section. We start by loading our dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from text_analytics import TextAnalytics\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ai = TextAnalytics()\n",
    "ai.data_dir = os.path.join(\".\", \"data\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the tweets that we need for each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              City                                               Text\n",
      "0       washington   you really need to go back to bar tending or ...\n",
      "1           london   jay finley christ in explains why today is co...\n",
      "2            lagos   forget if this happened truly it s definitely...\n",
      "3          toronto   yall i love this skin big thanks to for makin...\n",
      "4          nairobi   the late brilliant prof ali mazrui explains h...\n",
      "...            ...                                                ...\n",
      "150025     chennai   finally indian women team make victory in las...\n",
      "150026     chennai   syngene international ltd calls for board mee...\n",
      "150027     chennai   true no one takes you seriously such a senile...\n",
      "150028     chennai   do you really need to test your site to impro...\n",
      "150029     chennai   one becomes father of the nation and another ...\n",
      "\n",
      "[150030 rows x 2 columns]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "file = \"sociolinguistics.english_cities.gz\"\n",
    "file = os.path.join(ai.data_dir, file)\n",
    "df = pd.read_csv(file)\n",
    "print(df)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our content features are fit to the data in two ways: first, we learn phrases using pointwise mutual information; second, we learn TF-IDF weights by counting words across documents. Both of these steps can take some time. So we have pre-loaded the models we need. But, you can run the code below if you want to do it yourself (remove the comment tag first). You can check out the *text_analytics* package to see this in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHRASES:\n",
      "['united_states', 'boris_johnson', 'chuck_schumer', 'ricky_gervais', 'rashida_tlaib', 'hillary_clinton', 'devin_nunes', 'sherlock_holmes', 'edinson_cavani', 'climate_change', 'harry_potter', 'happy_birthday']\n",
      "\n",
      "VOCAB\n",
      "['people', 'new', 'today', 'need', 'want', 'video', 'life', 'world', 'year', 'please', 'watch', 'thanks']\n"
     ]
    }
   ],
   "source": [
    "#ai.fit_phrases(df, min_count=1, language=\"en\")\n",
    "#ai.fit_tfidf(df, n_features = 10000)\n",
    "\n",
    "file_phrases = os.path.join(ai.data_dir, \"sociolinguistics.english_all.gz\")\n",
    "ai.phrases = ai.deserialize(\"phrases\", file_phrases + \".phrases.json\")\n",
    "ai.tfidf_vectorizer = ai.deserialize(\"tfidf_model\", file + \".tfidf.json\")\n",
    "\n",
    "print(\"PHRASES:\")\n",
    "example_phrases = list(ai.phrases.phrasegrams.keys())\n",
    "print(example_phrases[:12])\n",
    "\n",
    "print(\"\\nVOCAB\")\n",
    "example_vocab = list(ai.tfidf_vectorizer.vocabulary_.keys())\n",
    "print(example_vocab[:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have (1) our data from Twitter and (2) our full content vectorizer (TF-IDF + PMI phrases). We're going to classify these by cities. The basic code is below; this just called our *text_analytics* package. That package splits the data into training and testing data, learns a classifier, and then evaluates the classifier. We're telling the package to use \"City\" as the ground-truth class, with content features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9726)\t0.0749937420567999\n",
      "  (0, 9658)\t0.07497708911780718\n",
      "  (0, 9090)\t0.07364115740517338\n",
      "  (0, 8973)\t0.0742041893027956\n",
      "  (0, 8645)\t0.07289270227746461\n",
      "  (0, 8327)\t0.07222838937187062\n",
      "  (0, 8291)\t0.0724010225801125\n",
      "  (0, 8092)\t0.0721626588101253\n",
      "  (0, 8043)\t0.0721103367495788\n",
      "  (0, 7998)\t0.07186492724429036\n",
      "  (0, 7980)\t0.07187771706160526\n",
      "  (0, 7853)\t0.07153716713149491\n",
      "  (0, 7660)\t0.07067434669250264\n",
      "  (0, 7623)\t0.07080215164329921\n",
      "  (0, 7439)\t0.07051366281361417\n",
      "  (0, 7078)\t0.06972018033040071\n",
      "  (0, 6990)\t0.06974151596970049\n",
      "  (0, 6941)\t0.1399358018127286\n",
      "  (0, 6928)\t0.06908731221807107\n",
      "  (0, 6552)\t0.06875858350367162\n",
      "  (0, 6512)\t0.06856372184379768\n",
      "  (0, 6508)\t0.06797932641569877\n",
      "  (0, 6464)\t0.06798852610842035\n",
      "  (0, 6230)\t0.06718801231956584\n",
      "  (0, 6206)\t0.0675637123540012\n",
      "  :\t:\n",
      "  (150029, 48)\t0.023706245439530375\n",
      "  (150029, 47)\t0.02252345595296407\n",
      "  (150029, 46)\t0.04476143061578029\n",
      "  (150029, 45)\t0.02245941325645736\n",
      "  (150029, 40)\t0.06812059389201963\n",
      "  (150029, 37)\t0.021483270101145046\n",
      "  (150029, 36)\t0.02128846174813955\n",
      "  (150029, 35)\t0.021202662232335398\n",
      "  (150029, 34)\t0.02081829627797108\n",
      "  (150029, 33)\t0.22836261702694766\n",
      "  (150029, 31)\t0.02086143531666654\n",
      "  (150029, 29)\t0.020442650215937292\n",
      "  (150029, 26)\t0.02030370089851719\n",
      "  (150029, 25)\t0.04048081536353046\n",
      "  (150029, 24)\t0.041699784426901323\n",
      "  (150029, 23)\t0.040981630386342296\n",
      "  (150029, 19)\t0.020692785393040935\n",
      "  (150029, 15)\t0.019329261718506924\n",
      "  (150029, 14)\t0.019277403240324793\n",
      "  (150029, 11)\t0.018861756979774123\n",
      "  (150029, 8)\t0.017862306877731562\n",
      "  (150029, 6)\t0.04882831655320408\n",
      "  (150029, 2)\t0.014299063369558909\n",
      "  (150029, 1)\t0.06569472690056806\n",
      "  (150029, 0)\t0.012849400221812517\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "x, vocab_size = ai.get_features(df, features = \"content\")\n",
    "print(x)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     adelaide       0.92      0.92      0.92       512\n",
      "      atlanta       0.92      0.92      0.92       502\n",
      "     auckland       0.99      0.99      0.99       528\n",
      "    bengaluru       0.96      0.93      0.95       503\n",
      "       boston       0.94      0.95      0.94       514\n",
      "     brisbane       0.88      0.87      0.87       515\n",
      "      calgary       1.00      0.98      0.99       529\n",
      "      chennai       0.99      1.00      0.99       491\n",
      "      chicago       0.92      0.91      0.91       498\n",
      "       dallas       0.93      0.93      0.93       501\n",
      "        delhi       0.93      0.95      0.94       487\n",
      " johannesburg       1.00      1.00      1.00       490\n",
      "      karachi       1.00      1.00      1.00       505\n",
      "      kolkata       0.96      0.95      0.95       481\n",
      "        lagos       1.00      1.00      1.00       501\n",
      "       london       0.97      0.97      0.97       509\n",
      "  los_angeles       0.86      0.92      0.89       490\n",
      "    melbourne       0.84      0.84      0.84       468\n",
      "        miami       0.97      0.93      0.95       485\n",
      "       mumbai       0.92      0.92      0.92       496\n",
      "      nairobi       1.00      1.00      1.00       518\n",
      "     new_york       0.91      0.91      0.91       517\n",
      "        perth       0.94      0.92      0.93       485\n",
      "      phoenix       0.97      0.95      0.96       496\n",
      "san_francisco       0.95      0.95      0.95       470\n",
      "      seattle       0.93      0.95      0.94       495\n",
      "    singapore       0.99      0.99      0.99       487\n",
      "      syndney       0.87      0.87      0.87       569\n",
      "      toronto       0.97      0.97      0.97       477\n",
      "   washington       0.95      0.96      0.96       484\n",
      "\n",
      "     accuracy                           0.94     15003\n",
      "    macro avg       0.95      0.94      0.94     15003\n",
      " weighted avg       0.94      0.94      0.94     15003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = ai.shallow_classification(df, labels = \"City\", features = \"content\", classifier = \"lm\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Be patient**\n",
    "\n",
    "And there we go! We're looking at the classifier accuracy. \n",
    "\n",
    "This will change a bit from the lecture, because we're using random train/test splits. That means the classifier is looking at different data each time. If you want more advanced examples for how to solve this city classification problem, take a look at the *text_analytics.shallow_classification()* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
